{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt import UtilityFunction\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Function\n",
    "\n",
    "Lets create a target 1-D function with multiple local maxima to test and visualize how the [BayesianOptimization](https://github.com/fmfn/BayesianOptimization) package works. The target function we will try to maximize is the following:\n",
    "\n",
    "$$f(x) = e^{-(x - 2)^2} + e^{-\\frac{(x - 6)^2}{10}} + \\frac{1}{x^2 + 1}, $$ its maximum is at $x = 2$ and we will restrict the interval of interest to $x \\in (-2, 10)$.\n",
    "\n",
    "Notice that, in practice, this function is unknown, the only information we have is obtained by sequentialy probing it at different points. Bayesian Optimization works by contructing a posterior distribution of functions that best fit the data observed and chosing the next probing point by balancing exploration and exploitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RHO_DEFAULT = 0.01\n",
    "M_DEFAULT = 1\n",
    "\n",
    "def target(x, y):\n",
    "    xs = np.array([x,y])\n",
    "    return np.array([np.sin(xs[0]) + xs[1]])\n",
    "\n",
    "def constraint(x, y):\n",
    "    z = np.array([x,y])\n",
    "    return np.sin(z[0])*np.sin(z[1]) + 0.95\n",
    "\n",
    "\n",
    "def u(x: np.array, z: np.array, y:np.array, rho = RHO_DEFAULT):\n",
    "    \"\"\"\n",
    "    params\n",
    "    z: np.array for constraint\n",
    "    x: np.array for value of x from k+1 iteration\n",
    "    y: np.array for lambda of lagrangian for kth iteration\n",
    "    rho: convergence parameter rho\n",
    "    M : some large number (hyperparameter) \n",
    "    \"\"\"\n",
    "    return target(x[0], x[1]) + q_i(z, x, y, rho, M = 1)\n",
    "    \n",
    "    \n",
    "\n",
    "def h_i(x: np.array , z: np.array,  y:np.array, rho = RHO_DEFAULT, M = M_DEFAULT):\n",
    "    \"\"\"\n",
    "    params\n",
    "    z: np.array for constraint\n",
    "    x: np.array for value of x from k+1 iteration\n",
    "    y: np.array for lambda of lagrangian for kth iteration\n",
    "    rho: convergence parameter rho\n",
    "    M : some large number (hyperparameter) \n",
    "    \"\"\"\n",
    "    return np.int64(constraint(z[0], z[1]) > 0) + q_i(z, x, y, rho, M)\n",
    "\n",
    "def q_i(x: np.array, z: np.array, y : np.float64, rho = 0.01, M = 1):\n",
    "    return rho / (2*M) * (np.linalg.norm(x - z + y/rho) ** 2)\n",
    "\n",
    "# def ei_constraint(z: np.array, x: np.array , theta: np.float64, rho = 0.01, M = 1):\n",
    "#     theta = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {'x': (0, 6), 'y': (0, 6)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a BayesianOptimization Object\n",
    "\n",
    "Enter the target function to be maximized, its variable(s) and their corresponding ranges. A minimum number of 2 initial guesses is necessary to kick start the algorithms, these can either be random or user defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_optim = BayesianOptimization(\n",
    "    f=target,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")\n",
    "c_optim = BayesianOptimization(\n",
    "    f=constraint,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#xi here is slack of utility function\n",
    "utility = UtilityFunction(kind=\"ei\", kappa=2.5, xi=1e-6)\n",
    "# c_utility = UtilityFunction(kind=\"constraint\", kappa=2.5, xi=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next point to probe is: {'x': 2.502132028215444, 'y': 4.321946960652949}\n",
      "Found the target value to be: [4.91870969]\n"
     ]
    }
   ],
   "source": [
    "next_point_to_probe = t_optim.suggest(utility)\n",
    "print(\"Next point to probe is:\", next_point_to_probe)\n",
    "\n",
    "t_val = target(**next_point_to_probe)\n",
    "print(\"Found the target value to be:\", t_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def ADMMBO(pbounds : dict, target : callable, constraints : np.array, regulariser : callable, \n",
    "           num_constraints : int, num_inits_f : int, num_inits_constraint : np.array,\n",
    "           init_ys : np.array = None, init_zs : np.array = None, init_xs : np.array = None,\n",
    "           rho : float = 1e-2, M : int = 1, epsilon : np.float64 = 1e-2, max_iter = 3):\n",
    "    \n",
    "    #Renaming a bit\n",
    "    n = num_inits_f\n",
    "    m = num_inits_constraint\n",
    "    \n",
    "    assert num_constraints == num_inits_constraint.shape[0]\n",
    "    \n",
    "    dim_space = len(pbounds)\n",
    "    \n",
    "    utility = UtilityFunction(kind=\"ei\", kappa=2.5, xi=1e-6)\n",
    "    \n",
    "    c_optims = []\n",
    "    for i in range(num_constraints):\n",
    "        c_optim = BayesianOptimization(\n",
    "                #FEAS here is without the regularizer\n",
    "                    f=constraints[i],\n",
    "                    pbounds=pbounds,\n",
    "                    verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "                    random_state=1,\n",
    "                )\n",
    "        c_optims.append(c_optim)\n",
    "    \n",
    "    \n",
    "    #Get initial points and evaluate\n",
    "    \n",
    "    #Initialise evaluation arrays\n",
    "    F = np.zeros(n)\n",
    "    Cs = np.zeros((num_constraints, np.max(m)))\n",
    "    if not init_xs:\n",
    "        init_xs = np.zeros((n, dim_space))\n",
    "        #Generate the initial x points and evaluate all these points\n",
    "        for i in range(n):\n",
    "            #The problem is this outputs a dictionary ...\n",
    "            x_i = t_optim.suggest(utility)\n",
    "            init_xs[i] = list(x_i.values())\n",
    "            F[i] = target(**x_i)\n",
    "    else:\n",
    "        assert init_xs.shape[0] == num_inits_f\n",
    "        assert init_xs.shape[1] == dim_space\n",
    "        \n",
    "        for i in range(n):\n",
    "            F[i] = target(init_xs[i])\n",
    "    \n",
    "    if not init_zs:\n",
    "        init_zs = np.zeros((num_constraints, np.max(m), dim_space))\n",
    "        # Generate but do not evaluate all the constraint points zs (as these are evaluated later)\n",
    "        for j in range(num_constraints):\n",
    "            for k in range(m[j]):\n",
    "                c_optim = c_optims[j]\n",
    "                z_i = c_optim.suggest(utility)\n",
    "                init_zs[j,k] = list(z_i.values())\n",
    "            \n",
    "            #Now register the points (this is done behind the scenes with probe. \n",
    "            #Lazy = True just saves some computation by only calculating inverses when necessary)\n",
    "            for k in range(m[j]):\n",
    "                point = init_zs[j,k]\n",
    "                Cs[j,k] = constraints[j](*point)\n",
    "                c_optim.probe(\n",
    "                        params = point,\n",
    "                        lazy = True\n",
    "                        )\n",
    "    else:\n",
    "        assert init_zs.shape[0] == num_constraints\n",
    "        assert init_zs.shape[1] == np.max(m)\n",
    "        assert init_zs.shape[2] == dim_space\n",
    "        assert False, \"not implemented yet\"\n",
    "    if not init_ys:\n",
    "        init_ys = np.random.uniform(0, 1, size = num_constraints)\n",
    "    else:\n",
    "        assert init_ys.shape[0] == num_constraints, \"number of initialised lambda values must be equal to number of constraints\"\n",
    "    \n",
    "    ##MAIN LOOP\n",
    "    \n",
    "    solved = False\n",
    "    k = 0\n",
    "    xs = init_xs\n",
    "    zs = init_zs\n",
    "    ys = init_ys\n",
    "    \n",
    "    #Use first slice of zs as current best value of zs\n",
    "    best_z = zs[:, 0, :]\n",
    "    \n",
    "    while k < max_iter and not solved:\n",
    "        best_x, xs, F = run_opt(target, regulariser, xs, F, pbounds, best_z, ys)\n",
    "        \n",
    "        best_z, zs, Cs, ys, solved = run_feas(\n",
    "            constraints=constraints, regulariser=regulariser, \n",
    "            z_mins_prev=best_z, m=m, x=best_x, \n",
    "            Cs=Cs, c_optims=c_optims, zs=zs, ys=ys, \n",
    "            pbounds=pbounds, rho=rho, M=M, epsilon=epsilon,\n",
    "            max_iter=1)\n",
    "        k+=1\n",
    "    return best_x, best_z\n",
    "\n",
    "#All good, does as expected\n",
    "def run_opt(target: callable, regulariser : callable, \n",
    "            xs : np.array, F : np.array, pbounds : dict, zs : np.array, ys : np.array, max_iter = 1):\n",
    "\n",
    "    assert len(ys) == len(zs)\n",
    "    \n",
    "    #We need a new t_optim function for each run because the posterior will be different based on each new z\n",
    "    t_optim = BayesianOptimization(\n",
    "            f=None,\n",
    "            pbounds=pbounds,\n",
    "            verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "            random_state=1,\n",
    "        )\n",
    "\n",
    "    U = np.zeros(len(xs))\n",
    "\n",
    "    #This updates the GP posterior\n",
    "    for i in range(xs.shape[0]):\n",
    "        q_sum = np.sum(np.array([regulariser(xs[i], zs[j], ys[j]) for j in range(zs.shape[0])]))\n",
    "        U[i] = F[i] + q_sum\n",
    "\n",
    "        #This here registers and runs the optimisation step, \n",
    "        #would be better if I could make this lazy but requires editing underlying code\n",
    "        \n",
    "        try:\n",
    "            t_optim.register(\n",
    "                params = xs[i],\n",
    "                target = U[i]\n",
    "            )\n",
    "        except:\n",
    "            print(f\"TARGET: we already have {xs[i]}, but it tried adding\")\n",
    "    \n",
    "    utility = UtilityFunction(kind=\"ei\", kappa=2.5, xi=1e-6) \n",
    "    for _ in range(max_iter):\n",
    "        next_point = t_optim.suggest(utility)\n",
    "\n",
    "        xs = np.concatenate((xs, np.array(list(next_point.values())).reshape(1, xs.shape[1])), axis = 0)\n",
    "        \n",
    "        F = np.concatenate((F, target(**next_point)))\n",
    "        \n",
    "        q_sum = np.sum(np.array([regulariser(xs[-1], zs[j], ys[j]) for j in range(zs.shape[0])]))\n",
    "        U[i] = F[-1] + q_sum\n",
    "        try:\n",
    "            t_optim.register(\n",
    "                params = xs[-1],\n",
    "                target = U[-1]\n",
    "            )\n",
    "        except:\n",
    "            print(f\"TARGET: we already have {xs[i]}, but it tried adding\")\n",
    "\n",
    "    argmin = np.argmin(U)\n",
    "    xmin = xs[argmin]\n",
    "\n",
    "    del t_optim\n",
    "\n",
    "    return (xmin, xs, F)\n",
    "    \n",
    "def run_feas(constraints : np.array, regulariser : callable, z_mins_prev : np.array, m : np.array,\n",
    "            x : np.array, Cs : np.array, c_optims : np.array, zs : np.array, ys : np.array, \n",
    "            pbounds : dict, rho : float, M : int, epsilon : float, max_iter = 1):\n",
    "\n",
    "\n",
    "    #Very unintuitive naming for rs and ss but it basically means the \"r's\" and \"s's\" \n",
    "    # used in the paper for later determining if we're done or not\n",
    "    z_mins = np.zeros((zs.shape[0], zs.shape[2])); rs = np.zeros((zs.shape[0], zs.shape[2])); \n",
    "    ss = np.zeros((zs.shape[0], zs.shape[2]));\n",
    "\n",
    "    #For each constraint\n",
    "    for j in range(zs.shape[0]):\n",
    "\n",
    "        c_optim = c_optims[j]\n",
    "\n",
    "        #TODO: Could probably be done more efficiently\n",
    "        #Evaluates H over all known points\n",
    "        H = (np.int64(Cs[j] > 0)\n",
    "                + np.array([regulariser(x, zs[j, a ,:], ys, rho, M) for a in range(zs.shape[1])]))\n",
    "        h_plus = np.max(H)\n",
    "        \n",
    "\n",
    "        #Might be able to do this more efficiently because its just minimising \n",
    "        # some norm so set z = x + y and truncate when out of bounds\n",
    "        utility = UtilityFunction(kind=\"constraint\", kappa=2.5, xi=1e-6, \n",
    "                                  x_constraint = x, y_constraint = ys[j], rho = rho, M = M, h_plus = h_plus)\n",
    "\n",
    "        for iteration in range(max_iter):\n",
    "            #Add something to h\n",
    "            #Could also use point = next_point\n",
    "            point = zs[j, -1, :]\n",
    "            h_eval = Cs[j, -1] - regulariser(x, point, ys[j], rho, M)\n",
    "            try:\n",
    "                c_optim.register(\n",
    "                    params = point,\n",
    "                    target = Cs[j, -1]\n",
    "                )\n",
    "            except:\n",
    "                print(f\"CONSTRAINT: tried adding {point} but looks like we already have that point\")\n",
    "\n",
    "            next_point = c_optim.suggest(utility)\n",
    "\n",
    "            #If we're at the max number of constraint m, we need to concat, otherwise just assign\n",
    "            if m[j] == np.max(m):\n",
    "                zs = np.concatenate((zs, np.zeros((zs.shape[0], 1, zs.shape[2]))), axis = 1)\n",
    "                Cs = np.concatenate((Cs, np.zeros((Cs.shape[0], 1))), axis = 1)\n",
    "  \n",
    "            zs[j, m[j]] = np.array(list(next_point.values()))\n",
    "            Cs[j, m[j]] = constraints[j](*list(next_point.values()))\n",
    "            \n",
    "           \n",
    "            H_new = int(Cs[j, -1] > 0) + regulariser(x, zs[j,-1,:], ys[j], rho, M)\n",
    "            H = np.concatenate((H, np.array([H_new])))\n",
    "        \n",
    "        z_mins[j, :] = zs[j, np.argmin(H), :]\n",
    "        rs[j, :] = x - z_mins[j, :] \n",
    "        ss[j, :] = -rho * (z_mins[j, :] - z_mins_prev[j, :])\n",
    "\n",
    "    is_solved = (np.linalg.norm(rs) < epsilon) & (np.linalg.norm(ss) < epsilon)\n",
    "    return z_mins, zs, Cs, ys, is_solved\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONSTRAINT: tried adding [0. 0.] but looks like we already have that point\n",
      "CONSTRAINT: tried adding [0. 0.] but looks like we already have that point\n",
      "CONSTRAINT: tried adding [0. 0.] but looks like we already have that point\n",
      "CONSTRAINT: tried adding [0. 0.] but looks like we already have that point\n",
      "CONSTRAINT: tried adding [0. 0.] but looks like we already have that point\n",
      "CONSTRAINT: tried adding [0. 0.] but looks like we already have that point\n",
      "CONSTRAINT: tried adding [0. 0.] but looks like we already have that point\n",
      "CONSTRAINT: tried adding [0. 0.] but looks like we already have that point\n",
      "CONSTRAINT: tried adding [0. 0.] but looks like we already have that point\n",
      "CONSTRAINT: tried adding [0. 0.] but looks like we already have that point\n",
      "CONSTRAINT: tried adding [0. 0.] but looks like we already have that point\n",
      "CONSTRAINT: tried adding [0. 0.] but looks like we already have that point\n",
      "CONSTRAINT: tried adding [0. 0.] but looks like we already have that point\n",
      "CONSTRAINT: tried adding [0. 0.] but looks like we already have that point\n",
      "CONSTRAINT: tried adding [0. 0.] but looks like we already have that point\n",
      "CONSTRAINT: tried adding [0. 0.] but looks like we already have that point\n",
      "CONSTRAINT: tried adding [0. 0.] but looks like we already have that point\n",
      "(array([0.01698159, 0.03396381]), array([[2.50213203, 4.32194696]]))\n"
     ]
    }
   ],
   "source": [
    "constraints = np.array([constraint])\n",
    "num_constraints = 1\n",
    "num_inits_f = 3\n",
    "num_inits_constraint = np.array([3])\n",
    "regulariser = q_i\n",
    "\n",
    "#Doesn't fkn converge ripppppp, they seem to diverge instead\n",
    "print(ADMMBO(pbounds, target, constraints, regulariser, num_constraints, num_inits_f, num_inits_constraint,\n",
    "           max_iter = 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
