{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt import UtilityFunction\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Function\n",
    "\n",
    "Lets create a target 1-D function with multiple local maxima to test and visualize how the [BayesianOptimization](https://github.com/fmfn/BayesianOptimization) package works. The target function we will try to maximize is the following:\n",
    "\n",
    "$$f(x) = e^{-(x - 2)^2} + e^{-\\frac{(x - 6)^2}{10}} + \\frac{1}{x^2 + 1}, $$ its maximum is at $x = 2$ and we will restrict the interval of interest to $x \\in (-2, 10)$.\n",
    "\n",
    "Notice that, in practice, this function is unknown, the only information we have is obtained by sequentialy probing it at different points. Bayesian Optimization works by contructing a posterior distribution of functions that best fit the data observed and chosing the next probing point by balancing exploration and exploitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RHO_DEFAULT = 0.01\n",
    "M_DEFAULT = 1\n",
    "\n",
    "def target(x, y):\n",
    "    return np.array([np.sin(x) + y])\n",
    "\n",
    "def constraint(x, y):\n",
    "    return np.sin(x)*np.sin(y) + 0.95\n",
    "\n",
    "\n",
    "def u(x: np.array, z: np.array, y:np.array, rho = RHO_DEFAULT):\n",
    "    \"\"\"\n",
    "    params\n",
    "    z: np.array for constraint\n",
    "    x: np.array for value of x from k+1 iteration\n",
    "    y: np.array for lambda of lagrangian for kth iteration\n",
    "    rho: convergence parameter rho\n",
    "    M : some large number (hyperparameter) \n",
    "    \"\"\"\n",
    "    return target(x[0], x[1]) + q_i(z, x, y, rho, M = 1)\n",
    "    \n",
    "    \n",
    "\n",
    "def h_i(x: np.array , z: np.array,  y:np.array, rho = RHO_DEFAULT, M = M_DEFAULT):\n",
    "    \"\"\"\n",
    "    params\n",
    "    z: np.array for constraint\n",
    "    x: np.array for value of x from k+1 iteration\n",
    "    y: np.array for lambda of lagrangian for kth iteration\n",
    "    rho: convergence parameter rho\n",
    "    M : some large number (hyperparameter) \n",
    "    \"\"\"\n",
    "    return np.int64(constraint(z[0], z[1]) > 0) + q_i(z, x, y, rho, M)\n",
    "\n",
    "def q_i(x: np.array, z: np.array, y : np.array, rho = 0.01, M = 1):\n",
    "    return rho / (2*M) * (np.linalg.norm(x - z + y/rho) ** 2)\n",
    "\n",
    "# def ei_constraint(z: np.array, x: np.array , theta: np.float64, rho = 0.01, M = 1):\n",
    "#     theta = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {'x': (0, 6), 'y': (0, 6)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a BayesianOptimization Object\n",
    "\n",
    "Enter the target function to be maximized, its variable(s) and their corresponding ranges. A minimum number of 2 initial guesses is necessary to kick start the algorithms, these can either be random or user defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_optim = BayesianOptimization(\n",
    "    f=target,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")\n",
    "c_optim = BayesianOptimization(\n",
    "    f=constraint,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#xi here is slack of utility function\n",
    "utility = UtilityFunction(kind=\"ei\", kappa=2.5, xi=1e-6)\n",
    "# c_utility = UtilityFunction(kind=\"constraint\", kappa=2.5, xi=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next point to probe is: {'x': 2.502132028215444, 'y': 4.321946960652949}\n",
      "Found the target value to be: [4.91870969]\n"
     ]
    }
   ],
   "source": [
    "next_point_to_probe = t_optim.suggest(utility)\n",
    "print(\"Next point to probe is:\", next_point_to_probe)\n",
    "\n",
    "t_val = target(**next_point_to_probe)\n",
    "print(\"Found the target value to be:\", t_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def ADMMBO(pbounds : dict, target : callable, constraints : np.array, regulariser : callable, \n",
    "           num_constraints : int, num_inits_f : int, num_inits_constraint : np.array,\n",
    "           init_ys : np.array = None, init_zs : np.array = None, init_xs : np.array = None,\n",
    "           rho : float = 1e-1, M : int = 20, epsilon : float = 0.05, max_iter_outer = 3,\n",
    "           max_iter_OPT : int = 5, max_iter_FEAS : int = 5):\n",
    "    \n",
    "    #Renaming a bit\n",
    "    n = num_inits_f\n",
    "    m = num_inits_constraint\n",
    "    \n",
    "    assert num_constraints == num_inits_constraint.shape[0]\n",
    "    \n",
    "    dim_space = len(pbounds)\n",
    "    \n",
    "    utility = UtilityFunction(kind=\"ei\", kappa=2.5, xi=1e-6)\n",
    "    \n",
    "    c_optims = []\n",
    "    for i in range(num_constraints):\n",
    "        c_optim = BayesianOptimization(\n",
    "                #FEAS here is without the regularizer\n",
    "                    f=constraints[i],\n",
    "                    pbounds=pbounds,\n",
    "                    verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "                    random_state=1,\n",
    "                )\n",
    "        c_optims.append(c_optim)\n",
    "    \n",
    "    \n",
    "    #Get initial points and evaluate\n",
    "    \n",
    "    #Initialise evaluation arrays\n",
    "    F = np.zeros(n)\n",
    "    Cs = np.zeros((num_constraints, np.max(m)))\n",
    "    if not init_xs:\n",
    "        init_xs = np.zeros((n, dim_space))\n",
    "        #Generate the initial x points and evaluate all these points\n",
    "        for i in range(n):\n",
    "            #The problem is this outputs a dictionary ...\n",
    "            x_i = t_optim.suggest(utility)\n",
    "            init_xs[i] = list(x_i.values())\n",
    "            F[i] = target(**x_i)\n",
    "    else:\n",
    "        assert init_xs.shape[0] == num_inits_f\n",
    "        assert init_xs.shape[1] == dim_space\n",
    "        \n",
    "        for i in range(n):\n",
    "            F[i] = target(init_xs[i])\n",
    "    \n",
    "    if not init_zs:\n",
    "        init_zs = np.zeros((num_constraints, np.max(m), dim_space))\n",
    "        # Generate but do not evaluate all the constraint points zs (as these are evaluated later)\n",
    "        for j in range(num_constraints):\n",
    "            for k in range(m[j]):\n",
    "                c_optim = c_optims[j]\n",
    "                z_i = c_optim.suggest(utility)\n",
    "                init_zs[j,k] = list(z_i.values())\n",
    "            \n",
    "            #Now register the points (this is done behind the scenes with probe. \n",
    "            #Lazy = True just saves some computation by only calculating inverses when necessary)\n",
    "            for k in range(m[j]):\n",
    "                point = init_zs[j,k]\n",
    "                Cs[j,k] = constraints[j](*point)\n",
    "                c_optim.register(\n",
    "                        params = point,\n",
    "                        target = Cs[j,k]\n",
    "                        )\n",
    "            c_optim._gp.fit(c_optim._space.params, c_optim._space.target)\n",
    "    else:\n",
    "        assert init_zs.shape[0] == num_constraints\n",
    "        assert init_zs.shape[1] == np.max(m)\n",
    "        assert init_zs.shape[2] == dim_space\n",
    "        assert False, \"not implemented yet\"\n",
    "    if not init_ys:\n",
    "        init_ys = np.random.uniform(0, 1, size = (num_constraints, dim_space))\n",
    "    else:\n",
    "        assert init_ys.shape[0] == num_constraints, \"number of initialised lambda values must be equal to number of constraints\"\n",
    "    \n",
    "    ##MAIN LOOP\n",
    "    \n",
    "    solved = False\n",
    "    k = 0\n",
    "    xs = init_xs\n",
    "    zs = init_zs\n",
    "    ys = init_ys\n",
    "    \n",
    "    #Use first slice of zs as current best value of zs\n",
    "    best_z = zs[:, 0, :]\n",
    "    \n",
    "    while k < max_iter_outer and not solved:\n",
    "        best_x, xs, F = run_opt(target, regulariser, xs, F, pbounds, best_z, ys, rho, max_iter = max_iter_OPT)\n",
    "        \n",
    "        best_z, zs, Cs, ys, c_optims, rho, solved = run_feas(\n",
    "            constraints=constraints, regulariser=regulariser, \n",
    "            z_mins_prev=best_z, m=m, x=best_x, \n",
    "            Cs=Cs, c_optims=c_optims, zs=zs, ys=ys, \n",
    "            pbounds=pbounds, rho=rho, M=M, epsilon=epsilon,\n",
    "            max_iter=max_iter_FEAS)\n",
    "        k+=1\n",
    "    \n",
    "    if solved:\n",
    "        return best_x, best_z, xs, F, zs, Cs\n",
    "    else:\n",
    "        return find_approx(xs=xs, F=F, zs=zs, Cs=Cs, c_optims=c_optims, pbounds = pbounds)\n",
    "            \n",
    "\n",
    "#All good, does as expected\n",
    "def run_opt(target : callable, regulariser : callable, xs : np.array, F : np.array, \n",
    "            pbounds : dict, zs : np.array, ys : np.array, rho : float, max_iter = 1):\n",
    "\n",
    "    assert len(ys) == len(zs)\n",
    "    \n",
    "    #We need a new t_optim function for each run because the posterior will be different based on each new z\n",
    "    temp_optim = BayesianOptimization(\n",
    "            f=None,\n",
    "            pbounds=pbounds,\n",
    "            verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "            random_state=1,\n",
    "        )\n",
    "\n",
    "    U = np.zeros(len(xs))\n",
    "\n",
    "    #This updates the GP posterior\n",
    "    for i in range(xs.shape[0]):\n",
    "        q_sum = np.sum(np.array([regulariser(xs[i], zs[j], ys[j], rho = rho) for j in range(zs.shape[0])]))\n",
    "        U[i] = F[i] + q_sum\n",
    "\n",
    "        #This here registers and runs the optimisation step, \n",
    "        #would be better if I could make this lazy but requires editing underlying code\n",
    "        \n",
    "        try:\n",
    "            temp_optim.register(\n",
    "                params = xs[i],\n",
    "                target = -U[i]\n",
    "            )\n",
    "        except:\n",
    "            print(f\"TARGET: we already have {xs[i]}, but it tried adding\")\n",
    "            assert False\n",
    "    \n",
    "    #Fit all the points added\n",
    "    temp_optim._gp.fit(temp_optim._space.params, temp_optim._space.target)\n",
    "    \n",
    "    utility = UtilityFunction(kind=\"ei\", kappa=2.5, xi=1e-6) \n",
    "    for _ in range(max_iter):\n",
    "        next_point = t_optim.suggest(utility)\n",
    "\n",
    "        xs = np.concatenate((xs, np.array(list(next_point.values())).reshape(1, xs.shape[1])), axis = 0)\n",
    "        \n",
    "        F = np.concatenate((F, target(**next_point)))\n",
    "        \n",
    "        q_sum = np.sum(np.array([regulariser(xs[-1], zs[j], ys[j], rho = rho) for j in range(zs.shape[0])]))\n",
    "        U[i] = F[-1] + q_sum\n",
    "        try:\n",
    "            #Will add -U[-1], else it will try and maximise, but we want to minimise\n",
    "            temp_optim.register(\n",
    "                params = next_point,\n",
    "                target = -U[-1]\n",
    "            )\n",
    "        except:\n",
    "            print(f\"TARGET: we already have {xs[i]}, but it tried adding\")\n",
    "\n",
    "    argmin = np.argmin(U)\n",
    "    xmin = xs[argmin]\n",
    "    \n",
    "    del temp_optim\n",
    "    return (xmin, xs, F)\n",
    "    \n",
    "def run_feas(constraints : np.array, regulariser : callable, z_mins_prev : np.array, m : np.array,\n",
    "            x : np.array, Cs : np.array, c_optims : np.array, zs : np.array, ys : np.array, \n",
    "            pbounds : dict, rho : float, M : int, epsilon : float, max_iter = 1):\n",
    "\n",
    "\n",
    "    #Very unintuitive naming for rs and ss but it basically means the \"r's\" and \"s's\" \n",
    "    # used in the paper for later determining if we're done or not\n",
    "    z_mins = np.zeros((zs.shape[0], zs.shape[2])); rs = np.zeros((zs.shape[0], zs.shape[2])); \n",
    "    ss = np.zeros((zs.shape[0], zs.shape[2]));\n",
    "\n",
    "    #For each constraint\n",
    "    for j in range(zs.shape[0]):\n",
    "\n",
    "        c_optim = c_optims[j]\n",
    "\n",
    "        #TODO: Could probably be done more efficiently\n",
    "        #Evaluates H over all known points\n",
    "        H = (np.int64(Cs[j] > 0)\n",
    "                + np.array([regulariser(x, zs[j, a ,:], ys[j], rho, M) for a in range(zs.shape[1])]))\n",
    "        h_plus = np.min(H)\n",
    "                \n",
    "\n",
    "        #Might be able to do this more efficiently because its just minimising \n",
    "        # some norm so set z = x + y and truncate when out of bounds\n",
    "        \n",
    "        utility = UtilityFunction(kind=\"constraint\", kappa=2.5, xi=1e-6, \n",
    "                                  x_constraint = x, y_constraint = ys[j], rho = rho, M = M, h_plus = h_plus)\n",
    "\n",
    "        \n",
    "        for iteration in range(max_iter):\n",
    "            #Add something to h\n",
    "            #If we're at the max number of constraint m, we need to concat, otherwise just assign\n",
    "            \n",
    "            next_point = c_optim.suggest(utility)\n",
    "            \n",
    "            if m[j] == np.max(m):\n",
    "                zs = np.concatenate((zs, np.zeros((zs.shape[0], 1, zs.shape[2]))), axis = 1)\n",
    "                Cs = np.concatenate((Cs, np.zeros((Cs.shape[0], 1))), axis = 1)\n",
    "                m[j]+=1\n",
    "  \n",
    "            zs[j, m[j]-1] = np.array(list(next_point.values()))\n",
    "            \n",
    "            #THIS IS CLEARLY NOT WORKING, returning 0 when should be returning 0.95 for example\n",
    "            Cs[j, m[j]-1] = constraints[j](*list(next_point.values()))\n",
    "\n",
    "            H_new = int(Cs[j, -1] > 0) + regulariser(x, zs[j,-1,:], ys[j], rho, M)\n",
    "            H = np.concatenate((H, np.array([H_new])))\n",
    "\n",
    "            try:\n",
    "                c_optim.register(\n",
    "                    params = next_point,\n",
    "                    target = Cs[j, -1]\n",
    "#                     target = H_new\n",
    "                )\n",
    "            except:\n",
    "                print(f\"CONSTRAINT: tried adding {next_point} but looks like we already have that point\")\n",
    "\n",
    "\n",
    "            \n",
    "        z_mins[j, :] = zs[j, np.argmin(H), :]\n",
    "        ys[j, :] += rho * (x - z_mins[j, :])\n",
    "        rs[j, :] = x - z_mins[j, :] \n",
    "        ss[j, :] = -rho * (z_mins[j, :] - z_mins_prev[j, :])\n",
    "    \n",
    "    is_solved = (np.linalg.norm(rs) < epsilon) & (np.linalg.norm(ss) < epsilon)\n",
    "    \n",
    "    mu = 10\n",
    "    tao = 2\n",
    "    \n",
    "    #Changing rho as recommended by the paper where we have hardcoded mu and tao here\n",
    "    if np.linalg.norm(rs) > mu * np.linalg.norm(ss):\n",
    "        rho*=tao\n",
    "    elif np.linalg.norm(rs) < mu * np.linalg.norm(ss):\n",
    "        rho/=tao\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    return z_mins, zs, Cs, ys, c_optims, rho, is_solved\n",
    "\n",
    "def find_approx(xs : np.array, F : np.array, zs : np.array, Cs: np.array, \n",
    "                c_optims : np.array, pbounds : dict, delta : float = 0.1):\n",
    "    \n",
    "    #Just using this object to get the GP mean later\n",
    "    final_optim = BayesianOptimization(\n",
    "            f=None,\n",
    "            pbounds=pbounds,\n",
    "            verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "            random_state=1,\n",
    "        )\n",
    "        \n",
    "    #Check through current xs to find candidate\n",
    "    \n",
    "    #Initialises\n",
    "    F_min = np.inf\n",
    "    approx_x_min = xs[0]\n",
    "    \n",
    "    #Sets the GP posterior\n",
    "    for i in range(xs.shape[0]):\n",
    "        final_optim.register(\n",
    "            params = xs[i],\n",
    "            target = F[i]\n",
    "        )\n",
    "        \n",
    "        #Check if it's a candidate for a minimum\n",
    "        if F[i] < F_min:\n",
    "            \n",
    "            #Check if high prob of satisifying constraint\n",
    "            satisfied = 1\n",
    "            for c_optim in c_optims:\n",
    "                mean, std = c_optim._gp.predict(xs[i].reshape(1,-1), return_std=True)\n",
    "                satisfied *= int(norm.cdf(-mean/std) > 1 - delta)\n",
    "            \n",
    "            #Only if all constraints are likely satisfied then update it as the minimum candidate\n",
    "            if satisfied == 1:\n",
    "                F_min = F[i]\n",
    "                approx_x_min = xs[i]\n",
    "    \n",
    "    \n",
    "    final_optim._gp.fit(final_optim._space.params, final_optim._space.target)\n",
    "    \n",
    "    #Check through zs\n",
    "    feasibles = zs[np.where(Cs < 0)]\n",
    "    for point in feasibles:\n",
    "        #If the expected value of the function at this point is better than our current optimum, update\n",
    "        E_f = final_optim._gp.predict(point.reshape(1,-1))\n",
    "        if E_f < F_min:\n",
    "            F_min = E_f\n",
    "            approx_x_min = point\n",
    "\n",
    "    #Return the best estimate and a bunch of debugging terms\n",
    "    print(F_min)\n",
    "    return approx_x_min, c_optims, xs, F, zs, Cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39436013]\n",
      "[4.62311052 1.39044669]\n"
     ]
    }
   ],
   "source": [
    "constraints = np.array([constraint])\n",
    "num_constraints = 1\n",
    "num_inits_f = 20\n",
    "num_inits_constraint = np.array([20])\n",
    "regulariser = q_i\n",
    "\n",
    "#Doesn't fkn converge ripppppp, they seem to diverge instead\n",
    "answers = ADMMBO(pbounds, target, constraints, regulariser, num_constraints, num_inits_f, \n",
    "                 num_inits_constraint, rho = 0.1, max_iter_outer=10, max_iter_OPT=20, max_iter_FEAS=20)\n",
    "x_min, c_optims, xs, F, zs, Cs = answers\n",
    "print(x_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.02986295626429869"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraint(*x_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39442936])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target(*x_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.013484239614913518"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraint(4.7, 1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30007674])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target(4.7, 1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pots = xs[(F<1) & (xs[:,0] > 4) & (xs[:,1] <2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3920794692807217\n",
      "0.33712742399249396\n",
      "-0.030968838008299526\n",
      "[0.40202036]\n",
      "[4.80379604 1.39784564]\n",
      "0.18335758957901294\n",
      "0.5687325606875853\n",
      "0.8628124293116438\n",
      "-0.036614126623246035\n",
      "[0.71745037]\n",
      "[4.79162155 1.71431311]\n",
      "0.6160217496354283\n",
      "0.055865542080778985\n",
      "0.8164179583349523\n",
      "0.7448057699245958\n",
      "0.1863314795295623\n",
      "0.6322896519964898\n",
      "0.7835783494011984\n",
      "0.26718598267267946\n"
     ]
    }
   ],
   "source": [
    "for point in pots:\n",
    "    check = constraint(*point)\n",
    "    print(check)\n",
    "    if check < 0 :\n",
    "        print(target(*point))\n",
    "        print(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.70534404, 1.47141456]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zs[0][np.argmin(Cs, axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 220)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.96269283, 0.09011388])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[np.argmin(F)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01044753]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
