{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt import UtilityFunction\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RHO_DEFAULT = 0.01\n",
    "M_DEFAULT = 1\n",
    "\n",
    "#Problem one\n",
    "def target(x, y):\n",
    "    return np.array([np.sin(x) + y])\n",
    "\n",
    "def constraint(x, y):\n",
    "    return np.sin(x)*np.sin(y) + 0.95\n",
    "\n",
    "#Problem 2\n",
    "def target_2(x, y):\n",
    "    return np.array([x + y])\n",
    "\n",
    "def constraint_2_1(x, y):\n",
    "    return -0.5 * np.sin(2*np.pi * (x**2 - 2*y)) - x - 2*y + 1.5\n",
    "\n",
    "def constraint_2_2(x, y):\n",
    "    return x**2 + y**2 - 1.5\n",
    "\n",
    "\n",
    "#YOUR OWN PROBLEM HERE\n",
    "def target_3(x, y):\n",
    "    raise NotImplemented\n",
    "def constraint_3(x,y):\n",
    "    return NotImplemented\n",
    "\n",
    "def regulariser(x: np.array, z: np.array, y : np.array, rho = 0.01, M = 1):\n",
    "    \n",
    "    return rho / (2*M) * (np.linalg.norm(x - z + y/rho) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import warnings\n",
    "\n",
    "    \n",
    "def ADMMBO(pbounds : dict, target : callable, constraints : np.array, regulariser : callable, \n",
    "           num_constraints : int, num_inits_f : int, num_inits_constraint : np.array,\n",
    "           init_ys : np.array = None, init_zs : np.array = None, init_xs : np.array = None,\n",
    "           rho : float = 1e-1, M : int = 20, epsilon : float = 0.05, max_iter_outer = 3,\n",
    "           max_iter_OPT : int = 5, max_iter_FEAS : int = 5):\n",
    "    \n",
    "    \"\"\"\n",
    "        params\n",
    "        z: np.array for constraint\n",
    "        x: np.array for value of x from k+1 iteration\n",
    "        y: np.array for lambda of lagrangian for kth iteration\n",
    "        rho: convergence parameter rho\n",
    "        M : some sufficiently large number (hyperparameter) \n",
    "    \"\"\"\n",
    "    #Renaming for brevity\n",
    "    n = num_inits_f\n",
    "    m = num_inits_constraint\n",
    "    \n",
    "    assert num_constraints == m.shape[0]\n",
    "    \n",
    "    \"\"\"\n",
    "                                                        INITIALISATION\n",
    "    ------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    \"\"\"\n",
    "    dim_space = len(pbounds)\n",
    "    \n",
    "    utility = UtilityFunction(kind=\"ei\", kappa=2.5, xi=1e-6)\n",
    "    \n",
    "    \n",
    "    #There will be one BayesianOptimisation function for each constraint, and these are saved in an array\n",
    "    c_optims = []\n",
    "    \n",
    "    for i in range(num_constraints):\n",
    "        new_c_optim = BayesianOptimization(\n",
    "                #Note the lack of the regulariser here, and therefore will require using a non-standard\n",
    "            #expectation-improvement procedure\n",
    "                    f=constraints[i],\n",
    "                    pbounds=pbounds,\n",
    "                    verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "                )\n",
    "        c_optims.append(new_c_optim)\n",
    "    \n",
    "    \n",
    "    #Initialise evaluation arrays (in paper F = {x, f(x)}, here F = {f(x)})\n",
    "        #Same goes for C : in paper, C = {z, c(z)}, here C = {c(z)}\n",
    "    F = np.zeros(n)\n",
    "    Cs = np.zeros((num_constraints, np.max(m)))\n",
    "    \n",
    "    #This is if xs were initialised outside the function called (not recommended currently!)\n",
    "    if not init_xs:\n",
    "        init_xs = np.zeros((n, dim_space))\n",
    "        #Generate the initial x points and evaluate all these points\n",
    "        for i in range(n):\n",
    "            #Because no points have been registered, this generates random points within bounds\n",
    "            x_i = c_optims[0].suggest(utility)\n",
    "            init_xs[i] = list(x_i.values())\n",
    "            F[i] = target(**x_i)\n",
    "    else:\n",
    "        assert init_xs.shape[0] == num_inits_f\n",
    "        assert init_xs.shape[1] == dim_space\n",
    "        \n",
    "        for i in range(n):\n",
    "            F[i] = target(init_xs[i])\n",
    "    \n",
    "    if not init_zs:\n",
    "        init_zs = np.zeros((num_constraints, np.max(m), dim_space))\n",
    "        # Generate but do not evaluate all the constraint points zs (as these are evaluated later)\n",
    "        for j in range(num_constraints):\n",
    "            for k in range(m[j]):\n",
    "                c_optim = c_optims[j]\n",
    "                z_i = c_optim.suggest(utility)\n",
    "                init_zs[j,k] = list(z_i.values())\n",
    "            \n",
    "            #Now register the points (this is done behind the scenes with probe. \n",
    "            #Lazy = True just saves some computation by only calculating inverses when necessary)\n",
    "            for k in range(m[j]):\n",
    "                point = init_zs[j,k]\n",
    "                Cs[j,k] = constraints[j](*point)\n",
    "                c_optim.register(\n",
    "                        params = point,\n",
    "                        target = Cs[j,k]\n",
    "                        )\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                c_optim._gp.fit(c_optim._space.params, c_optim._space.target)\n",
    "    else:\n",
    "        assert init_zs.shape[0] == num_constraints\n",
    "        assert init_zs.shape[1] == np.max(m)\n",
    "        assert init_zs.shape[2] == dim_space\n",
    "        assert False, \"not implemented yet\"\n",
    "    if not init_ys:\n",
    "        init_ys = np.random.uniform(0, 1, size = (num_constraints, dim_space))\n",
    "    else:\n",
    "        assert init_ys.shape[0] == num_constraints, \"number of initialised lambda values must be equal to number of constraints\"\n",
    "    \n",
    "    \"\"\"\n",
    "                                                        MAIN LOOP\n",
    "    ------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    solved = False\n",
    "    k = 0\n",
    "    xs = init_xs\n",
    "    zs = init_zs\n",
    "    ys = init_ys\n",
    "    \n",
    "    #Use first slice of zs as current best value of zs\n",
    "    best_z = zs[:, 0, :]\n",
    "    \n",
    "    while k < max_iter_outer and not solved:\n",
    "        best_x, xs, F = run_opt(target, regulariser, xs, F, pbounds, best_z, ys, rho, max_iter = max_iter_OPT)\n",
    "        \n",
    "        best_z, zs, Cs, ys, c_optims, m, rho, solved = run_feas(\n",
    "            constraints=constraints, regulariser=regulariser, \n",
    "            z_mins_prev=best_z, m=m, x=best_x, \n",
    "            Cs=Cs, c_optims=c_optims, zs=zs, ys=ys, \n",
    "            pbounds=pbounds, rho=rho, M=M, epsilon=epsilon,\n",
    "            max_iter=max_iter_FEAS)\n",
    "        k+=1\n",
    "    \n",
    "    if solved:\n",
    "        return best_x\n",
    "    else:\n",
    "        print('Did not stop at criterion, finding approximate answer ... ')\n",
    "        return find_approx(xs=xs, F=F, zs=zs, Cs=Cs, c_optims=c_optims, pbounds = pbounds)\n",
    "            \n",
    "\n",
    "def run_opt(target : callable, regulariser : callable, xs : np.array, F : np.array, \n",
    "            pbounds : dict, zs : np.array, ys : np.array, rho : float, max_iter = 1):\n",
    "    \"\"\"\n",
    "                                                       OPT INNER LOOPS\n",
    "    ------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    \"\"\"\n",
    "    assert len(ys) == len(zs)\n",
    "    \n",
    "    #We need a new temp_optim object for each run because the posterior will be different based on each new z\n",
    "    temp_optim = BayesianOptimization(\n",
    "            f=None,\n",
    "            pbounds=pbounds\n",
    "        )\n",
    "\n",
    "    U = np.zeros(len(xs))\n",
    "\n",
    "    #This updates the GP posterior\n",
    "    for i in range(xs.shape[0]):\n",
    "        q_sum = np.sum(np.array([regulariser(xs[i], zs[j], ys[j], rho = rho) for j in range(zs.shape[0])]))\n",
    "        U[i] = F[i] + q_sum\n",
    "\n",
    "        #This here registers the points without fitting the GP\n",
    "        try:\n",
    "            temp_optim.register(\n",
    "                params = xs[i],\n",
    "                target = -U[i]\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "            #This is fine, and should only be uncommented if interested in knowing \n",
    "            #print(f\"TARGET: we already have {xs[i]}, but it tried adding\")\n",
    "    \n",
    "    #Fit all the points added\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        temp_optim._gp.fit(temp_optim._space.params, temp_optim._space.target)\n",
    "    \n",
    "    utility = UtilityFunction(kind=\"ei\", kappa=2.5, xi=1e-6) \n",
    "    for _ in range(max_iter):\n",
    "        next_point = temp_optim.suggest(utility)\n",
    "\n",
    "        xs = np.concatenate((xs, np.array(list(next_point.values())).reshape(1, xs.shape[1])), axis = 0)\n",
    "        \n",
    "        F = np.concatenate((F, target(**next_point)))\n",
    "        \n",
    "        q_sum = np.sum(np.array([regulariser(xs[-1], zs[j], ys[j], rho = rho) for j in range(zs.shape[0])]))\n",
    "        U[i] = F[-1] + q_sum\n",
    "        try:\n",
    "            #Will add -U[-1], else it will try and maximise, but we want to minimise\n",
    "            temp_optim.register(\n",
    "                params = next_point,\n",
    "                target = -U[-1]\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "            #This is fine, and should only be uncommented if interested in knowing \n",
    "            #print(f\"TARGET: we already have {xs[i]}, but it tried adding\")\n",
    "    \n",
    "    #Find the best value after all inner loops are completed\n",
    "    argmin = np.argmin(U)\n",
    "    xmin = xs[argmin]\n",
    "    \n",
    "    #This isn't needed anymore, so save some memory\n",
    "    del temp_optim\n",
    "    \n",
    "    return (xmin, xs, F)\n",
    "    \n",
    "def run_feas(constraints : np.array, regulariser : callable, z_mins_prev : np.array, m : np.array,\n",
    "            x : np.array, Cs : np.array, c_optims : np.array, zs : np.array, ys : np.array, \n",
    "            pbounds : dict, rho : float, M : int, epsilon : float, max_iter = 1):\n",
    "    \"\"\"\n",
    "                                                      FEAS INNER LOOPS\n",
    "    ------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #Very unintuitive naming for rs and ss but it basically means the \"r's\" and \"s's\" \n",
    "    # used in the paper for later determining if we're done or not\n",
    "    z_mins = np.zeros((zs.shape[0], zs.shape[2])); rs = np.zeros((zs.shape[0], zs.shape[2])); \n",
    "    ss = np.zeros((zs.shape[0], zs.shape[2]));\n",
    "\n",
    "    #For each constraint\n",
    "    for j in range(zs.shape[0]):\n",
    "        c_optim = c_optims[j]\n",
    "        #Evaluates H over all known points\n",
    "        H = (np.int64(Cs[j][0:m[j]] > 0)\n",
    "                + np.array([regulariser(x, zs[j, a ,:], ys[j], rho, M) for a in range(m[j])]))\n",
    "        h_plus = np.min(H)\n",
    "        \n",
    "        utility = UtilityFunction(kind=\"constraint\", kappa=None, xi=0, \n",
    "                                  x_constraint = x, y_constraint = ys[j], rho = rho, M = M, h_plus = h_plus)\n",
    "        \n",
    "        for iteration in range(max_iter):\n",
    "            \n",
    "            #Add something to h\n",
    "            #If we're at the max number of constraint m, we need to concat, otherwise just assign\n",
    "            next_point = c_optim.suggest(utility)\n",
    "            \n",
    "            if m[j] == np.max(m):\n",
    "                zs = np.concatenate((zs, np.zeros((zs.shape[0], 1, zs.shape[2]))), axis = 1)\n",
    "                Cs = np.concatenate((Cs, np.zeros((Cs.shape[0], 1))), axis = 1)\n",
    "            \n",
    "            m[j]+=1\n",
    "            zs[j, m[j]-1] = np.array(list(next_point.values()))\n",
    "            c_eval = constraints[j](*list(next_point.values()))\n",
    "            Cs[j, m[j]-1] = c_eval         \n",
    "\n",
    "            H_new = int(c_eval > 0) + regulariser(x, zs[j, m[j]-1, :], ys[j], rho, M)\n",
    "            \n",
    "            if m[j]-1 == H.shape[0]:\n",
    "                H = np.concatenate((H, np.array([H_new])))\n",
    "            else:\n",
    "                H[m[j]-1] = H_new\n",
    "            \n",
    "            try:\n",
    "                c_optim.register(\n",
    "                    params = next_point,\n",
    "                    target = c_eval\n",
    "                )\n",
    "            except:\n",
    "                pass\n",
    "                #This is fine, and should only be uncommented if interested in knowing \n",
    "            #print(f\"CONSTRAINT: tried adding {next_point} but looks like we already have that point\")        \n",
    "        \n",
    "        #Update the parameters for the next outer loop iteration\n",
    "        z_mins[j, :] = zs[j, np.argmin(H), :]\n",
    "        ys[j, :] += rho * (x - z_mins[j, :])\n",
    "        rs[j, :] = x - z_mins[j, :] \n",
    "        ss[j, :] = -rho * (z_mins[j, :] - z_mins_prev[j, :])\n",
    "    \n",
    "    is_solved = (np.linalg.norm(rs) < epsilon) & (np.linalg.norm(ss) < epsilon)\n",
    "    \n",
    "    \n",
    "    #Changing rho as recommended by the paper where we have hardcoded mu and tao here\n",
    "    mu = 10\n",
    "    tao = 2\n",
    "    \n",
    "    if np.linalg.norm(rs) > mu * np.linalg.norm(ss):\n",
    "        rho*=tao\n",
    "    elif np.linalg.norm(rs) < mu * np.linalg.norm(ss):\n",
    "        rho/=tao\n",
    "    else:\n",
    "        pass\n",
    "    return z_mins, zs, Cs, ys, c_optims, m, rho, is_solved\n",
    "\n",
    "def find_approx(xs : np.array, F : np.array, zs : np.array, Cs: np.array, \n",
    "                c_optims : np.array, pbounds : dict, delta : float = 0.1):\n",
    "    \"\"\"\n",
    "                                          FIND APPROXIMATE ANSWER IF STOP CRITERION NOT MET\n",
    "    ------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    \"\"\"    \n",
    "    #Just using this object to get the GP mean later\n",
    "    final_optim = BayesianOptimization(\n",
    "            f=None,\n",
    "            pbounds=pbounds\n",
    "        )\n",
    "        \n",
    "    #Check through current xs to find candidate\n",
    "    \n",
    "    #Initialises\n",
    "    F_min = np.inf\n",
    "    approx_x_min = xs[0]\n",
    "    \n",
    "    #Sets the GP posterior\n",
    "    for i in range(xs.shape[0]):\n",
    "        try:\n",
    "            final_optim.register(\n",
    "                params = xs[i],\n",
    "                target = F[i]\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        #Check if it's a candidate for a minimum\n",
    "        if F[i] < F_min:\n",
    "            \n",
    "            #Check if high prob of satisifying constraint\n",
    "            satisfied = 1\n",
    "            for c_optim in c_optims:\n",
    "                mean, std = c_optim._gp.predict(xs[i].reshape(1,-1), return_std=True)\n",
    "                satisfied *= int(norm.cdf(-mean/std) > 1 - delta)\n",
    "            \n",
    "            #Only if all constraints are likely satisfied then update it as the minimum candidate\n",
    "            if satisfied == 1:\n",
    "                F_min = F[i]\n",
    "                approx_x_min = xs[i]\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        final_optim._gp.fit(final_optim._space.params, final_optim._space.target)\n",
    "    \n",
    "    #Check through zs\n",
    "    feasibles = zs[np.where(Cs < 0)]\n",
    "    for point in feasibles:\n",
    "        #If the expected value of the function at this point is better than our current optimum, update\n",
    "        E_f = final_optim._gp.predict(point.reshape(1,-1))\n",
    "        satisfied = 1\n",
    "        for c_optim in c_optims:\n",
    "            mean, std = c_optim._gp.predict(xs[i].reshape(1,-1), return_std=True)\n",
    "            satisfied *= int(norm.cdf(-mean/std) > 1 - delta)\n",
    "        \n",
    "        if E_f < F_min and satisfied == 1:\n",
    "            F_min = E_f\n",
    "            approx_x_min = point\n",
    "\n",
    "    if F_min == np.inf:\n",
    "        print(\"no feasible point was found in the search, try increasing no. of iterations\")\n",
    "        return None\n",
    "    del final_optim\n",
    "    #Return the best estimate\n",
    "    return approx_x_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Subproblem 1:\n",
    "I.e target = $\\min_{x,y \\in B = [0,6]^2} \\sin(x) + y$\n",
    "\n",
    "subject to the constraint $\\sin(x)*\\sin(y) + 0.95 \\leq 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.71960024 1.30745788]\n"
     ]
    }
   ],
   "source": [
    "constraints = np.array([constraint])\n",
    "num_constraints = 1\n",
    "num_inits_f = 20\n",
    "num_inits_constraint = np.array([20])\n",
    "pbounds = {'x': (0, 6), 'y': (0, 6)}\n",
    "\n",
    "answer = ADMMBO(pbounds, target, constraints, regulariser, num_constraints, num_inits_f, \n",
    "                 num_inits_constraint, rho = 0.1, max_iter_outer=80, max_iter_OPT=5, max_iter_FEAS=5)\n",
    "x_min = answer\n",
    "print(x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if constraints were indeed satisfied and value of the target funciton at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of the constraint at the found solution was -0.015501239409989154 and was therefore satisfied\n",
      "The value of the target function was 0.3074838786799571 which is higher than that found by the paper, with a solution of 0.3000767424358992\n"
     ]
    }
   ],
   "source": [
    "paper_answer_1 = np.array([4.7, 1.3])\n",
    "print(\"The value of the constraint at the found solution was \" + str(constraint(*x_min)) + \n",
    "        \" and was therefore \" + (\"not \" if constraint(*x_min) > 0 else \"\") + \"satisfied\")\n",
    "\n",
    "print(\"The value of the target function was \" + str(target(*x_min)[0]) + \" which is \" + \n",
    "      (\"lower \" if target(*x_min) < target(*paper_answer_1) else \"higher \") + \"than that found by the paper, with a solution of \" + str(target(*paper_answer_1)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Subproblem 2:\n",
    "I.e target = $\\min_{x,y \\in B = [0,1]^2} x + y$\n",
    "\n",
    "subject to the constraints:\n",
    "\n",
    "$-0.5 * \\sin(2 \\pi(x^2 - 2y)) - x - 2y + 1.5 \\leq 0$\n",
    "\n",
    "$ x^2 + y^2 - 1.5 \\leq 0$\n",
    "\n",
    "We run our iterations a fewer number of times than the paper did, just for demonstration purposes. Feel free to increase the number of iterations.\n",
    "\n",
    "One may need to restart the kernel as the underlying library stores a lot in cache and needs to be reset sometimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not stop at criterion, finding approximate answer ... \n",
      "[0.20245288 0.43318168]\n"
     ]
    }
   ],
   "source": [
    "constraints = np.array([constraint_2_1, constraint_2_2])\n",
    "num_constraints = 2\n",
    "num_inits_f = 10\n",
    "num_inits_constraint = np.array([10,10])\n",
    "pbounds = {'x': (0, 1), 'y': (0, 1)}\n",
    "answer = ADMMBO(pbounds, target_2, constraints, regulariser, num_constraints, num_inits_f, \n",
    "                 num_inits_constraint, rho = 0.1, max_iter_outer=30, max_iter_OPT=5, max_iter_FEAS=5)\n",
    "x_min = answer\n",
    "print(x_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of the first constraint at the found solution was -0.013781722832890031 and was therefore satisfied\n",
      "The value of the second constraint at the found solution was -1.2713664624918577 and was therefore satisfied\n",
      "The value of the target function was 0.6342544001462149 whereas the paper found [0.19  0.415] which gives a value of 0.6038588949765006\n"
     ]
    }
   ],
   "source": [
    "paper_answer_2 = np.array([0.19, 0.415])\n",
    "print(\"The value of the first constraint at the found solution was \" + str(constraint_2_1(*x_min)) + \n",
    "        \" and was therefore \" + (\"not \" if constraint_2_1(*x_min) > 0 else \"\") + \"satisfied\")\n",
    "\n",
    "print(\"The value of the second constraint at the found solution was \" + str(constraint_2_2(*x_min)) + \n",
    "        \" and was therefore \" + (\"not \" if constraint_2_2(*x_min) > 0 else \"\") + \"satisfied\")\n",
    "\n",
    "print(\"The value of the target function was \" + str(target(*x_min)[0]) + \" whereas the paper found \" \n",
    "      + str(paper_answer_2) + \" which gives a value of \" + str(target(*paper_answer_2)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.31141383, 0.47308905])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04113417016370513"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
